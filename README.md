# TVÂ³S: Exploiting Temporal State Space Sharing for Video Semantic Segmentation

[![PyTorch](https://img.shields.io/badge/framework-PyTorch-brightgreen.svg)]()
[![License](https://img.shields.io/badge/license-Creative%20Commons%20BY--NC--SA%204.0-blue.svg)](LICENSE)
[![arXiv](https://img.shields.io/badge/arXiv-2503.20824-red.svg)](https://arxiv.org/abs/2503.20824)

**Exploiting Temporal State Space Sharing for Video Semantic Segmentation**  
Official PyTorch implementation of TVÂ³Sâ€”Temporal Video State Space Sharing.

---

## ğŸ“– Table of Contents

- [âœ¨ Features](#-features)  
- [ğŸ“ Introduction](#-introduction)  
- [ğŸ¨ Architecture](#-architecture)  
- [ğŸ‘¥ Authors](#-authors)  
- [ğŸš€ Installation](#-installation)  
- [ğŸ¬ Quick Start](#-quick-start)  
- [ğŸ› ï¸ Usage](#-%EF%B8%8F-usage)  
  - [Data Preparation](#data-preparation)  
  - [Inference](#inference)  
  - [Training](#training)  
- [ğŸ“Š Results](#-results)  
- [âœ… TODO](#-todo)
- [ğŸ“ Citation](#-citation)  
- [ğŸ¤ Acknowledgements](#-acknowledgements)  
- [ğŸ“¬ Contact](#-contact)  

---

## âœ¨ Features

- **Efficient Temporal State Sharing**  
    Utilizes Mamba state space models to seamlessly propagate temporal features across frames, reducing computational overhead.  

- **Scalable Patch-Based Processing**  
    Processes spatial patches independently, enabling parallelization for enhanced throughput and scalability.  

- **Adaptive Temporal Gating**  
    Dynamically selects relevant temporal cues to ensure long-range coherence and robust segmentation.  

- **State-of-the-Art Results**  
    Achieves superior performance on benchmarks like VSPW and Cityscapes, delivering an optimal balance between accuracy and efficiency.  


---
## ğŸ“ Introduction

Video semantic segmentation (VSS) plays a crucial role in understanding dynamic scenes over time. Conventional methods often process each frame or short clips independently, leading to fragmented context and high memory overhead. TVÂ³S (Temporal Video State Space Sharing) addresses these limitations through innovative strategies:

1. **Temporal Feature Sharing**: Leverages a compact state space model (Mamba) to propagate temporal features efficiently across frames.  
2. **Shifted-Window Motion Capture**: Utilizes a shifted-window mechanism to accurately model motion at patch boundaries.  
3. **Coherence-Driven Inference**: Incorporates past-frame information during inference to ensure long-term consistency and robust segmentation.  

These advancements enable scalable, parallel processing of extended video sequences while achieving state-of-the-art accuracy and efficiency.

---

## ğŸ¨ Architecture

![TV3S Full Block Diagram](resources/TV3S_fullblock.png)

---

## ğŸ‘¥ Authors

- [Syed Hesham](https://www.linkedin.com/in/syedhesham/)  
- [Yun Liu](https://yun-liu.github.io/)  
- [Guolei Sun](https://scholar.google.com/citations?hl=zh-CN&user=qd8Blw0AAAAJ)  
- [Henghui Ding](https://henghuiding.github.io/)  
- [Jing Yang](https://scholar.google.com/citations?user=jw6fQloAAAAJ&hl=zh-CN)  
- [Ender Konukoglu](https://scholar.google.com/citations?user=OeEMrhQAAAAJ&hl=en)  
- [Xue Geng](https://scholar.google.com.sg/citations?user=ZYVZ1bgAAAAJ&hl=en)  
- [Xudong Jiang](https://scholar.google.com.sg/citations?user=IL3mSioAAAAJ&hl=en)  

---
## ğŸš€ Installation

### Prerequisites

Ensure the following dependencies are installed:

```text
Python 3.10.16  
PyTorch 2.2.0  
torchvision 0.17.0  
CUDA 12.1  
timm 0.4.12  
mmcv-full 1.3.0  
opencv-python 4.8.1  
numpy 1.26.4  
yapf 0.40.1  
```

### Step 1: Install MMSegmentation

Follow the [MMSegmentation v0.11.0 installation guide](https://github.com/open-mmlab/mmsegmentation/blob/v0.11.0/docs/get_started.md#installation) to set up `mmseg`.

### Step 2: Install Mamba

Compile `mamba` from source using the instructions provided in [Mamba v1.0.0](https://github.com/state-spaces/mamba/tree/v1.0.0).

### Step 3: Clone the Repository

Clone the TVÂ³S repository:

```bash
git clone https://github.com/Ashesham/TV3S.git
```
For additional help with installation, please refer to [moreinfo.md](moreinfo.md).

---

## ğŸ¬ Quick Start

### Data Preparation

Download VSPW 480p and organize:

```bash
VSPW_480p
    â”œâ”€â”€ data
    |    â”œâ”€â”€ video1
    |    |    â”œâ”€â”€ origin
    |    |    |    â”œâ”€â”€ .jpg
    |    |    |    ...
    |    |    â””â”€â”€ mask
    |    |        â”œâ”€â”€ .png
    |    |        ...
    |    |
    |    â”œâ”€â”€ video2
    |    ...
    |    
    â”œâ”€â”€  data.txt
    ...    
  ```

Create a symbolic link to the VSPW dataset:

```bash
cd TV3S
mkdir -p data/vspw/
ln -s /path/to/VSPW_480p data/vspw/

```
---

## ğŸ› ï¸ Usage

### Inference

1. Download pretrained TVÂ³S weights:
   [OneDrive link](https://entuedu-my.sharepoint.com/:f:/g/personal/syedhesh002_e_ntu_edu_sg/EtZx0Emq4ZhMiIqeeSPqV_0Bx-YDelkgZb-lihVFKVMq5Q?e=Z3WqFQ)
2. Multi-GPU test (2 GPUs):

   ```bash
   ./tools/dist_test.sh \
     local_configs/tv3s/B1/tv3s.b1.480x480.vspw2.160k.py \
     /path/to/checkpoint.pth 2 \
     --out results/res.pkl
   ```

### Training

1. Place ImageNet-1K pretrained SegFormer weights in `pretrained/`.
2. Launch training (â‰¥2 GPUs, â‰¥20 GB each):

   ```bash
   ./tools/dist_train.sh \
     local_configs/tv3s/B1/tv3s.b1.480x480.vspw2.160k.py \
     2 --work-dir work_dirs/vspw2_4g
   ```

---

## ğŸ“Š Results

### VSPW Benchmark

| Method            | Backbone   | mIoU â†‘   | mVCâ‚ˆ â†‘   | mVCâ‚â‚† â†‘   | FPS â†‘    |
|------------------- |-----------|----------|----------|-----------|----------|
| SegFormerâ€  \[68]  | MiT-B1    | 36.5     | 84.7     | 79.9      | 58.7     |
| CFFM \[57]        | MiT-B1    | 38.5     | 88.6     | 84.1      | 29.8     |
| MRCFA \[58]       | MiT-B1    | 38.9     | 88.8     | 84.4      | 40.1     |
| **TVÂ³S (Ours)**   | **MiT-B1**| **40.0** | **90.7** | **87.0**  | **24.7** |
| SegFormerâ€  \[68]  | MiT-B2    | 43.9     | 86.0     | 81.2      | 16.2     |
| CFFM \[57]        | MiT-B2    | 44.9     | 89.8     | 85.8      | 10.1     |
| MRCFA \[58]       | MiT-B2    | 45.3     | 90.3     | 86.2      | 10.7     |
| **TVÂ³S (Ours)**   | **MiT-B2**| **46.3** | **91.5** | **88.35** | **21.9** |
| Mask2Formerâ€  \[7] | Swin-T    | 41.2     | 84.5     | 80.0      | 17.1     |
| MPVSS \[67]       | Swin-T    | 39.9     | 85.9     | 80.4      | 32.8     |
| **TVÂ³S (Ours)**   | **Swin-T**| **44.9** | **88.0** | **83.5**  | **22.9** |
| Mask2Formerâ€  \[7] | Swin-S    | 42.1     | 84.7     | 79.3      | 14.5     |
| MPVSS \[67]       | Swin-S    | 40.4     | 86.0     | 80.7      | 30.6     |
| **TVÂ³S (Ours)**   | **Swin-S**| **50.6** | **89.6** | **85.8**  | **19.5** |

### Cityscapes Benchmark

| Method            | Backbone   | mIoU â†‘   | GFLOPs â†“ | Params (M) â†“ | FPS â†‘    |
|------------------- |-----------|----------|----------|--------------|----------|
| SegFormerâ€  \[67]  | MiT-B1    | 74.1     | â€“        | 13.8         | 46.8     |
| CFFM \[56]        | MiT-B1    | 75.1     | 158.7    | 15.4         | 11.7     |
| MRCFA \[57]       | MiT-B1    | 75.1     | 145.0    | 14.9         | 13.0     |
| **TVÂ³S (Ours)**   | **MiT-B1**| **75.6** | **83.6** | **17.3**     | **25.1** |

---

## âœ… TODO

> **ğŸ“Œ Upcoming Tasks**  
> - [âœ”ï¸] Publish inference code for TVÂ³S.  
> - [âœ”ï¸] Publish training code for TVÂ³S.  
> - [ ] Publish inference code for continuous video segmentation.  

---

## ğŸ“ Citation

```bibtex
@inproceedings{hesham2025exploiting,
  title={Exploiting Temporal State Space Sharing for Video Semantic Segmentation},
  author={Hesham, Syed Ariff Syed and Liu, Yun and Sun, Guolei and Ding, Henghui and Yang, Jing and Konukoglu, Ender and Geng, Xue and Jiang, Xudong},
  booktitle={CVPR},
  year={2025}
}
```

---

## ğŸ¤ Acknowledgements

Built on top of:

* [VSS-CFFM by Guolei Sun](https://github.com/GuoleiSun/VSS-CFFM)
* [MMSegmentation](https://github.com/open-mmlab/mmsegmentation)
* [VideoMamba](https://github.com/OpenGVLab/VideoMamba)
* [SegFormer by NVlabs](https://github.com/NVlabs/SegFormer)
* [Focal Transformer by Microsoft](https://github.com/microsoft/Focal-Transformer)

---

## ğŸ“¬ Contact

For any questions or issues, please raise an issue on GitHub or contact the authors directly:
* Syed Hesham: [syedhesh002@e.ntu.edu.sg](mailto:syedhesh002@e.ntu.edu.sg)
* Yun Liu: [liuyun@nankai.edu.cn](mailto:liuyun@nankai.edu.cn)
* Guolei Sun: [sunguolei.kaust@gmail.com](mailto:sunguolei.kaust@gmail.com)

